# Model-Based-RL

Model-based reinforcement learning (RL) methods are a promising solution to robotic control tasks. They offer higher sample efficiency than model-free RL methods. In addition, model-based RL allows for dynamic model simulation of behaviors prior to enacting them, thereby improving safety and reducing robot wear and tear. An issue with model-based RL is that the transition model must be expressive enough to ensure low model bias. We sought to address this problem by investigating the impact of the choice of dynamics model. We looked at neural network models, which are universal function approximators but require many samples to train. In comparison, we evaluated Gaussian Process (GP) regression models, which are non- parametric and provides estimates on model uncertainty. Finally, we looked at how ensemble methods could be used to further improve model fit and performance. Our experimental evaluations show that the neural networks performed better than Gaussian Process regression. We used both modeling approaches in a model predictive control framework to evaluate model performance. We explored different hyperparameters for each modeling approach: the number of sampled action sequences for the neural network, and the number of local models/clusters evaluated for the GP model. Testing it on different Mujoco tasks such as Half-Cheetah and Reacher showed that neural networks were more suitable for fitting the complex Mujoco dynamic systems models.
